<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Background</title>
  <style>
    * {
      box-sizing: border-box;
    }

    html, body {
      margin: 0;
      padding: 0;
      height: 100%;
      width: 100%;
      font-family: Arial, sans-serif;
    }

    .container {
      display: flex;
      flex-direction: column;
      height: 100%;
      width: 100%;
      border: 3px solid black;
    }

    .header {
      height: 15vh;
      min-height: 80px;
      width: 100%;
      background-image: url("{{ url_for('static', filename='back.jpg') }}");
      background-size: cover;
      background-position: center;
      border-bottom: 3px solid black;
    }

    .nav {
      display: flex;
      flex-wrap: wrap;
      justify-content: space-around;
      align-items: center;
      border-bottom: 3px solid black;
      padding: 10px 0;
      font-weight: bold;
    }

    .nav a {
      text-decoration: none;
      color: black;
      padding: 10px 20px;
      margin: 5px;
      border: 2px solid black;
      border-radius: 5px;
      background-color: #f0f0f0;
      transition: background-color 0.3s;
    }

    .nav a:hover {
      background-color: #ddd;
    }

    .section {
      flex: 1;
      overflow-y: auto;
      padding: 2vw 4vw;
      max-width: 900px;
      margin: 0 auto;
      text-align: left;
    }

    ul {
      margin-left: 20px;
    }

    @media (max-width: 600px) {
      .header {
        font-size: 1.5rem;
      }

      .nav {
        flex-direction: column;
      }

      .nav a {
        width: 90%;
        text-align: center;
      }

      .section {
        padding: 5vw;
      }
    }
  </style>
</head>
<body>
  <div class="container">
    <div class="header"></div> <!-- background image only -->

    <div class="nav">
      <a href="{{ url_for('background') }}">Background</a>
      <a href="{{ url_for('statistics') }}">Statistics</a>
      <a href="{{ url_for('prediction') }}">Prediction</a>
    </div>

    <div class="section">
      <h3>Data Source</h3>
      <p>
        The dataset used in this project is the <strong>Ocular Disease Recognition (ODIR-5K)</strong> dataset. 
        It was originally sourced from the <a href="https://odir2019.grand-challenge.org/" target="_blank" rel="noopener noreferrer">ODIR Grand Challenge</a> 
        and hosted publicly on 
        <a href="https://www.kaggle.com/datasets/andrewmvd/ocular-disease-recognition-odir5k/data" target="_blank" rel="noopener noreferrer">Kaggle</a>.
        The dataset includes fundus images and metadata (age, sex, diagnostic keywords, and final labels) from more than 5,000 patients.
      </p>

      <h3>Tech Stack</h3>
      <p>
        The system utilizes <strong>Apache Spark</strong> for large-scale data processing and feature engineering. Feature vectors are 
        constructed from both structured metadata and image-based data:
      </p>
      <ul>
        <li><strong>Demographics:</strong> Age (binned), Sex (one-hot encoded)</li>
        <li><strong>Image Features:</strong> ResNet embeddings (from pre-trained CNN), color histograms (OpenCV), and word2vec or BERT embeddings (for diagnostic text)</li>
      </ul>
      <p>
        A combined feature vector is passed into a TensorFlow-based Multilayer Perceptron (MLP) classifier. Flask serves as the backend framework
        for exposing prediction endpoints, interfacing with a MySQL database, and rendering the UI.
      </p>

      <h3>Label Grouping</h3>
      <p>
        Due to class imbalance and multi-label complexity, we simplified the final diagnosis labels into broader categories:
      </p>
      <ul>
        <li><strong>N:</strong> Normal</li>
        <li><strong>RV:</strong> Retinal Abnormalities (e.g., Diabetes ['D'], Hypertension ['H'])</li>
        <li><strong>ON:</strong> Optic Nerve-related (e.g., Glaucoma ['G'])</li>
        <li><strong>L:</strong> Lens-related (e.g., Cataract ['C'])</li>
        <li><strong>MR:</strong> Macular/Retinal Degeneration (e.g., Age-related ['A'], Myopia-related ['M'])</li>
        <li><strong>O:</strong> Other/Unspecified</li>
      </ul>
      <p>
        These classes are encoded and used for supervised learning, yielding improved model convergence and interpretability.
      </p>


      <h3>Model Training</h3>
      <p>
        The core model is a TensorFlow-based Multilayer Perceptron (MLP) trained on a combined feature set derived from fundus images and associated metadata. The training process involves the following steps:
      </p>

      <ol>
        <li>
          <strong>Data Loading and Preprocessing</strong><br/>
          The dataset is loaded from a CSV file containing image-derived features (<code>resnet_features</code>, <code>histogram_features</code>) and text embeddings (<code>word2vec_features</code>).<br/>
          Feature columns stored as JSON strings or string arrays are parsed and converted into NumPy arrays.<br/>
          Rows with any missing feature data are dropped to ensure consistent input size.<br/>
          The <code>resnet_features</code> are normalized using <code>StandardScaler</code> to zero-mean and unit variance.
        </li>

        <li>
          <strong>Feature Combination</strong><br/>
          After normalization, the image features and text embeddings are horizontally stacked into a single feature matrix.
        </li>

        <li>
          <strong>Label Mapping and Encoding</strong><br/>
          Original diagnosis labels are mapped into broader diagnostic groups (e.g., Normal, Retinal Abnormalities, Optic Nerve-related).<br/>
          These groups are encoded as integer labels using <code>LabelEncoder</code> for model compatibility.
        </li>

        <li>
          <strong>Train-Test Split</strong><br/>
          The dataset is split into training (80%) and testing (20%) subsets, with a fixed random seed for reproducibility.
        </li>

        <li>
          <strong>Model Architecture</strong><br/>
          The MLP consists of an input layer matching the combined feature dimension, followed by:<br/>
          <ul>
            <li>Gaussian noise layer to improve generalization.</li>
            <li>Three dense layers with ReLU activation, He-normal initialization, L2 regularization, batch normalization, and dropout for regularization.</li>
            <li>A final dense output layer with softmax activation for multi-class classification.</li>
          </ul>
        </li>

        <li>
          <strong>Training</strong><br/>
          The model is compiled with Adam optimizer and sparse categorical cross-entropy loss.<br/>
          Trained for 20 epochs with a small batch size (5) and 10% validation split.<br/>
          Training leverages GPU acceleration when available for performance optimization.
        </li>

        <li>
          <strong>Evaluation and Persistence</strong><br/>
          Model accuracy is evaluated on the held-out test set.<br/>
          The trained model and the feature scaler are saved for later inference and deployment.
        </li>
      </ol>

      <p>
        This comprehensive training pipeline enables robust multi-modal classification of ocular disease categories, improving diagnostic support through advanced feature engineering and deep learning techniques.
      </p>
      <h3>Project Timeline</h3>
      <p>
        The project was conducted over three phases:
      </p>
      <ul>
        <li><strong>Phase 1:</strong> Data acquisition, Spark setup, and proposal (April)</li>
        <li><strong>Phase 2:</strong> Feature extraction, augmentation, and model training (May)</li>
        <li><strong>Phase 3:</strong> Flask-based web interface, model integration, and deployment (Mayâ€“June)</li>
      </ul>
    </div>
  </div>
</body>
</html>